{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLm8hD4yluk9"
      },
      "outputs": [],
      "source": [
        "\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import time, re, json, csv, sys\n",
        "from dataclasses import dataclass\n",
        "from typing import Iterator, Optional, Dict, Any, List\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    page_url_pattern: str\n",
        "    item_selector: str\n",
        "    title_selector: Optional[str]\n",
        "    price_selector: Optional[str]\n",
        "    url_selector: Optional[str]\n",
        "    next_selector: Optional[str] = None\n",
        "\n",
        "    def parse_price(self, text: Optional[str]) -> Optional[float]:\n",
        "        if not text:\n",
        "            return None\n",
        "        m = re.search(r\"([0-9]+(?:[.,][0-9]+)?)\", text)\n",
        "        return float(m.group(1).replace(\",\", \".\")) if m else None\n"
      ],
      "metadata": {
        "id": "vOlyqVJomAVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QUOTES_CFG = Config(\n",
        "    page_url_pattern=\"https://quotes.toscrape.com/page/{page}/\",\n",
        "    item_selector=\".quote\",\n",
        "    title_selector=\".text\",\n",
        "    price_selector=None,\n",
        "    url_selector=\"span a\",\n",
        "    next_selector=\".next a\"\n",
        ")\n",
        "\n",
        "# DLA BOOKS: używamy \"next\", więc ważny jest next_selector i start_url = index.html\n",
        "BOOKS_CFG = Config(\n",
        "    page_url_pattern=\"\",  # w trybie next nieużywane\n",
        "    item_selector=\"article.product_pod\",\n",
        "    title_selector=\"h3 a\",\n",
        "    price_selector=\".price_color\",\n",
        "    url_selector=\"h3 a\",\n",
        "    next_selector=\"li.next a\"\n",
        ")"
      ],
      "metadata": {
        "id": "AaPW99qRmPC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- HTTP --------------------\n",
        "\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (compatible; PaginatorScraper/1.1)\"}\n",
        "\n",
        "def get(url: str, retries: int = 3, backoff: float = 0.5) -> requests.Response:\n",
        "    last_exc = None\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            r = requests.get(url, headers=HEADERS, timeout=15)\n",
        "            r.raise_for_status()\n",
        "            return r\n",
        "        except requests.RequestException as e:\n",
        "            last_exc = e\n",
        "            if i < retries - 1:\n",
        "                time.sleep(backoff * (i + 1))\n",
        "    raise last_exc  # type: ignore\n",
        "\n",
        "def text_or_none(node) -> Optional[str]:\n",
        "    return node.get_text(\" \", strip=True) if node else None\n",
        "\n",
        "def abs_href(root: BeautifulSoup, el) -> Optional[str]:\n",
        "    if not el:\n",
        "        return None\n",
        "    href = el.get(\"href\")\n",
        "    if not href:\n",
        "        return None\n",
        "    base_tag = root.select_one(\"base[href]\")\n",
        "    base_url = base_tag.get(\"href\") if base_tag else \"\"\n",
        "    return urljoin(base_url, href)"
      ],
      "metadata": {
        "id": "EIefR-kcmgqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- ITERACJA --------------------\n",
        "\n",
        "def iterate_by_pattern(cfg: Config, start_page: int, max_pages: int) -> Iterator[str]:\n",
        "    for p in range(start_page, start_page + max_pages):\n",
        "        yield cfg.page_url_pattern.format(page=p)\n",
        "\n",
        "def iterate_by_next(start_url: str, cfg: Config, max_pages: int) -> Iterator[str]:\n",
        "    url = start_url\n",
        "    pages = 0\n",
        "    while url and pages < max_pages:\n",
        "        yield url\n",
        "        pages += 1\n",
        "        soup = BeautifulSoup(get(url).text, \"html.parser\")\n",
        "        nxt = soup.select_one(cfg.next_selector) if cfg.next_selector else None\n",
        "        url = urljoin(url, nxt.get(\"href\")) if (nxt and nxt.get(\"href\")) else None\n"
      ],
      "metadata": {
        "id": "RW2uFBZGnB-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- EKSTRAKCJA --------------------\n",
        "\n",
        "def extract_items(page_html: str, cfg: Config) -> List[Dict[str, Any]]:\n",
        "    soup = BeautifulSoup(page_html, \"htmlparser\") if False else BeautifulSoup(page_html, \"html.parser\")\n",
        "    out: List[Dict[str, Any]] = []\n",
        "    for item in soup.select(cfg.item_selector):\n",
        "        # tytuł\n",
        "        title = None\n",
        "        if cfg.title_selector:\n",
        "            tnode = item.select_one(cfg.title_selector)\n",
        "            title = tnode.get(\"title\") if (tnode and tnode.get(\"title\")) else text_or_none(tnode)\n",
        "        # cena\n",
        "        price_val = None; price_raw = None\n",
        "        if cfg.price_selector:\n",
        "            pnode = item.select_one(cfg.price_selector)\n",
        "            price_raw = text_or_none(pnode)\n",
        "            price_val = cfg.parse_price(price_raw)\n",
        "        # link\n",
        "        link_abs = None\n",
        "        if cfg.url_selector:\n",
        "            anode = item.select_one(cfg.url_selector)\n",
        "            link_abs = abs_href(soup, anode)\n",
        "        out.append({\"title\": title, \"price\": price_val, \"price_raw\": price_raw, \"url\": link_abs})\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "hGzNd8cFnt-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- ZAPIS --------------------\n",
        "\n",
        "def save_csv(rows: List[Dict[str, Any]], path: str) -> None:\n",
        "    if not rows:\n",
        "        print(f\"Brak danych → {path} nie zapisany.\")\n",
        "        return\n",
        "    keys = set().union(*(r.keys() for r in rows))\n",
        "    cols = sorted(keys)\n",
        "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=cols)\n",
        "        w.writeheader()\n",
        "        for r in rows:\n",
        "            w.writerow(r)\n",
        "    print(f\"CSV zapisany: {path} ({len(rows)} rekordów)\")\n",
        "\n",
        "def save_jsonl(rows: List[Dict[str, Any]], path: str) -> None:\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in rows:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "    print(f\"JSONL zapisany: {path} ({len(rows)} rekordów)\")\n"
      ],
      "metadata": {
        "id": "zlZVdHbwoCNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- RUN --------------------\n",
        "\n",
        "def run(mode: str, cfg: Config, start_url: Optional[str] = None,\n",
        "        start_page: int = 1, max_pages: int = 50, delay: float = 0.3) -> List[Dict[str, Any]]:\n",
        "    results: List[Dict[str, Any]] = []\n",
        "\n",
        "    if mode == \"pattern\":\n",
        "        urls = list(iterate_by_pattern(cfg, start_page, max_pages))\n",
        "    elif mode == \"next\":\n",
        "        if not start_url:\n",
        "            raise ValueError(\"Dla mode='next' wymagany jest start_url.\")\n",
        "        urls = list(iterate_by_next(start_url, cfg, max_pages))\n",
        "    else:\n",
        "        raise ValueError(\"mode ∈ {'pattern','next'}\")\n",
        "\n",
        "    print(f\"Do odwiedzenia {len(urls)} stron...\")\n",
        "    for i, url in enumerate(urls, 1):\n",
        "        try:\n",
        "            r = get(url)\n",
        "            page_rows = extract_items(r.text, cfg)\n",
        "            if not page_rows and mode == \"pattern\":\n",
        "                print(f\"[{i}] {url} – pusto, przerywam.\")\n",
        "                break\n",
        "            results.extend(page_rows)\n",
        "            print(f\"[{i}] {url} → {len(page_rows)} wyników (suma: {len(results)})\")\n",
        "            time.sleep(delay)\n",
        "        except Exception as e:\n",
        "            print(f\"[{i}] Błąd przy {url}: {e}\", file=sys.stderr)\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "azF9SIw5oU2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # DEMO 1: quotes (działa jak wcześniej)\n",
        "    quotes = run(\n",
        "        mode=\"next\",\n",
        "        cfg=QUOTES_CFG,\n",
        "        start_url=\"https://quotes.toscrape.com/page/1/\",\n",
        "        max_pages=50,\n",
        "        delay=0.2\n",
        "    )\n",
        "    save_csv(quotes, \"quotes_results.csv\")\n",
        "    save_jsonl(quotes, \"quotes_results.jsonl\")\n",
        "\n",
        "    # DEMO 2: books – KLUCZOWA ZMIANA → tryb 'next' + start od index.html\n",
        "    books = run(\n",
        "        mode=\"next\",\n",
        "        cfg=BOOKS_CFG,\n",
        "        start_url=\"https://books.toscrape.com/catalogue/category/books/travel_2/index.html\",\n",
        "        max_pages=50,\n",
        "        delay=0.2\n",
        "    )\n",
        "    save_csv(books, \"books_results.csv\")\n",
        "    save_jsonl(books, \"books_results.jsonl\")\n",
        "\n",
        "    print(\"Gotowe.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkZVEEHGoz0R",
        "outputId": "8f222c70-975b-4f72-b953-38dc3386bbe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do odwiedzenia 10 stron...\n",
            "[1] https://quotes.toscrape.com/page/1/ → 10 wyników (suma: 10)\n",
            "[2] https://quotes.toscrape.com/page/2/ → 10 wyników (suma: 20)\n",
            "[3] https://quotes.toscrape.com/page/3/ → 10 wyników (suma: 30)\n",
            "[4] https://quotes.toscrape.com/page/4/ → 10 wyników (suma: 40)\n",
            "[5] https://quotes.toscrape.com/page/5/ → 10 wyników (suma: 50)\n",
            "[6] https://quotes.toscrape.com/page/6/ → 10 wyników (suma: 60)\n",
            "[7] https://quotes.toscrape.com/page/7/ → 10 wyników (suma: 70)\n",
            "[8] https://quotes.toscrape.com/page/8/ → 10 wyników (suma: 80)\n",
            "[9] https://quotes.toscrape.com/page/9/ → 10 wyników (suma: 90)\n",
            "[10] https://quotes.toscrape.com/page/10/ → 10 wyników (suma: 100)\n",
            "CSV zapisany: quotes_results.csv (100 rekordów)\n",
            "JSONL zapisany: quotes_results.jsonl (100 rekordów)\n",
            "Do odwiedzenia 1 stron...\n",
            "[1] https://books.toscrape.com/catalogue/category/books/travel_2/index.html → 11 wyników (suma: 11)\n",
            "CSV zapisany: books_results.csv (11 rekordów)\n",
            "JSONL zapisany: books_results.jsonl (11 rekordów)\n",
            "Gotowe.\n"
          ]
        }
      ]
    }
  ]
}